# LLM Interface Environment Variables
# Copy this file to .env and fill in your actual API keys

# ============================================================================
# PROVIDER SELECTION
# ============================================================================
# Choose which LLM provider to use: "huggingface", "groq", or "googleaistudio"
LLM_PROVIDER=groq

# ============================================================================
# API KEYS
# ============================================================================
# HuggingFace API Token
# Get from: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=your_huggingface_token_here

# Groq API Key (FAST inference, generous free tier)
# Get from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# Google AI Studio API Key (Gemini models, generous free tier)
# Get from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
# Model name (provider-specific format)
#
# For Groq:
#   - llama-3.3-70b-versatile (RECOMMENDED - latest, best instruction following)
#   - llama-3.1-70b-versatile
#   - mixtral-8x7b-32768
#
# For Google AI Studio:
#   - gemini-2.0-flash-exp (RECOMMENDED - latest experimental)
#   - gemini-2.0-flash (stable version)
#   - gemini-2.5-flash (preview)
#   - gemini-2.5-pro (preview)
#
# For HuggingFace:
#   - meta-llama/Llama-3.3-70B-Instruct
#   - meta-llama/Llama-3.1-8B-Instruct
#
LLM_MODEL=llama-3.3-70b-versatile

# ============================================================================
# GENERATION PARAMETERS
# ============================================================================
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1000
LLM_TIMEOUT=30
