# ASL-v1: American Sign Language Recognition System

A comprehensive sign language recognition system using pose estimation and deep learning models for translating ASL videos to English text.

## ğŸ¯ Features

- **Multiple Model Architectures**: OpenHands-Modernized, Transformer, CNN-LSTM
- **Flexible Class Support**: Train on 20, 50, or 100 sign classes
- **Real-time Translation**: Webcam support for live sign language translation
- **Data Augmentation**: Advanced augmentation pipeline for improved model performance
- **End-to-End Pipeline**: Video â†’ Pose â†’ Segmentation â†’ Prediction â†’ Sentence Construction
- **Centralized Configuration**: Easy path management for multi-machine setup

## ğŸ“ Project Structure

```
asl-v1/
â”œâ”€â”€ config/                      # Configuration system
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ paths.py                # Path configuration module
â”‚   â”œâ”€â”€ settings.json           # User-specific paths (UPDATE THIS!)
â”‚   â””â”€â”€ MIGRATION_GUIDE.md      # Configuration guide
â”‚
â”œâ”€â”€ models/                      # Model architectures and training
â”‚   â”œâ”€â”€ openhands-modernized/   # OpenHands model implementation
â”‚   â”œâ”€â”€ Transformer/            # Transformer-based model
â”‚   â”œâ”€â”€ CNN-LSTM/               # CNN-LSTM model
â”‚   â””â”€â”€ training-scripts/       # Training scripts
â”‚       â””â”€â”€ train_asl.py        # Main training script
â”‚
â”œâ”€â”€ dataset-utilities/          # Dataset processing tools
â”‚   â”œâ”€â”€ augmentation/           # Data augmentation scripts
â”‚   â”œâ”€â”€ conversion/             # Format conversion utilities
â”‚   â”œâ”€â”€ dataset-splitting/      # Train/val/test splitting
â”‚   â”œâ”€â”€ segmentation/           # Sign segmentation tools
â”‚   â””â”€â”€ visualization/          # Visualization utilities
â”‚
â”œâ”€â”€ applications/               # End-user applications
â”‚   â”œâ”€â”€ predict_sentence.py    # Video to sentence translation
â”‚   â”œâ”€â”€ predict_sentence_with_gemini_streaming.py
â”‚   â””â”€â”€ motion_based_segmenter.py
â”‚
â”œâ”€â”€ project-utilities/          # Helper utilities
â”‚
â”œâ”€â”€ archive/                    # Legacy experiments (not maintained)
â”‚
â””â”€â”€ setup_config.py             # Interactive configuration setup

```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd asl-v1
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Paths

**Option A: Interactive Setup (Recommended)**
```bash
python setup_config.py
```

**Option B: Manual Configuration**

Edit `config/settings.json`:
```json
{
  "data_root": "/path/to/your/dataset-root",
  "project_root": "auto"
}
```

**Verify Configuration:**
```bash
python -m config.paths
```

### 4. Prepare Your Dataset

Your dataset root should contain:
- `pickle_files/` - Pose data in pickle format
- `pose_files/` - Pose files
- `dataset_splits/` - Train/val/test splits
- `class_index_mapping_XX.json` - Class mapping files
- `video_to_gloss_mapping.json` - Video metadata

See [config/MIGRATION_GUIDE.md](config/MIGRATION_GUIDE.md) for details.

## ğŸ‹ï¸ Training

### Train on 20 Classes (Default)
```bash
python models/training-scripts/train_asl.py --classes 20 --dataset original
```

### Train on 50 Classes with Augmented Data
```bash
python models/training-scripts/train_asl.py --classes 50 --dataset augmented
```

### Advanced Training Options
```bash
python models/training-scripts/train_asl.py \
  --classes 20 \
  --dataset augmented \
  --architecture openhands \
  --model-size large \
  --early-stopping 15 \
  --force-fresh
```

**Options:**
- `--classes`: Number of classes (20, 50, 100)
- `--dataset`: Dataset type (original, augmented)
- `--architecture`: Model type (openhands, transformer)
- `--model-size`: Model size (small, large)
- `--early-stopping`: Early stopping patience
- `--force-fresh`: Start fresh training (ignore checkpoints)

### Test Trained Model
```bash
python models/training-scripts/train_asl.py --classes 20 --mode test
```

## ğŸ¥ Inference

### Video to Sentence Translation
```bash
python applications/predict_sentence.py input_video.mp4 \
  --checkpoint ./models/wlasl_20_class_model \
  --gemini-api-key YOUR_API_KEY
```

### Real-time Webcam Translation
```bash
python applications/predict_sentence.py --webcam \
  --checkpoint ./models/wlasl_20_class_model \
  --gemini-api-key YOUR_API_KEY
```

**Options:**
- `--checkpoint`: Path to trained model checkpoint
- `--gemini-api-key`: Gemini API key for sentence construction (optional)
- `--segmentation-method`: Segmentation method (auto, motion)
- `--use-top-k`: Use top-k predictions (1-5)

## ğŸ› ï¸ Dataset Utilities

### Split Dataset
```bash
# Split pose files into train/val/test
python dataset-utilities/dataset-splitting/split_pose_files_nclass.py --num-classes 20
```

### Generate Augmented Dataset
```bash
python dataset-utilities/augmentation/generate_augmented_dataset.py \
  --source /path/to/pickle_files \
  --target /path/to/output
```

### Convert Video to Pose
```bash
python dataset-utilities/conversion/video_to_pose_extraction.py
```

## ğŸ“Š Research Project Timeline

This project follows a phased research approach, progressing from isolated sign recognition to a complete real-time translation system.

| Phase | Title | Status | Planned Completion | Key Deliverables | Success Criteria |
|-------|-------|--------|-------------------|------------------|------------------|
| **Phase 1** | Isolated Sign Recognition<br>(OpenHands-Modernized) | ğŸ”„ **IN PROGRESS** | TBD | â€¢ Trained 20-class model<br>â€¢ Trained 50-class model<br>â€¢ Architecture paper | â€¢ 80%+ Top-3 accuracy (20-class)<br>â€¢ 60%+ Top-3 accuracy (50-class)<br>â€¢ Published baseline results |
| **Phase 2** | LLM Sentence Constructor | â³ NOT STARTED | TBD | â€¢ LLM integration module<br>â€¢ Sign-to-sentence pipeline<br>â€¢ Grammar correction system | â€¢ Grammatically correct sentences<br>â€¢ Context-based disambiguation<br>â€¢ 90%+ sentence coherence |
| **Phase 3** | Full Pipeline Integration | â³ NOT STARTED | TBD | â€¢ End-to-end system<br>â€¢ Batch processing capability<br>â€¢ Evaluation framework | â€¢ Functional sign sequence â†’ English text<br>â€¢ <2s latency per sign<br>â€¢ 75%+ translation accuracy |
| **Phase 4** | Continuous Sign Detection | â³ NOT STARTED | TBD | â€¢ Temporal segmentation model<br>â€¢ Boundary detection system<br>â€¢ Continuous recognition pipeline | â€¢ 85%+ boundary detection accuracy<br>â€¢ Real-time processing capability<br>â€¢ <200ms segmentation latency |
| **Phase 5** | Real-Time Webcam Application | â³ NOT STARTED | TBD | â€¢ Desktop application<br>â€¢ Real-time inference pipeline<br>â€¢ User interface | â€¢ 15-30 FPS processing<br>â€¢ <500ms end-to-end latency<br>â€¢ Deployable application |
| **Phase 6** | Optimization & Deployment | â³ NOT STARTED | TBD | â€¢ Model quantization<br>â€¢ Performance optimization<br>â€¢ Documentation & demos | â€¢ 2x speed improvement<br>â€¢ Production-ready code<br>â€¢ Public release |

## ğŸ“Š Model Performance

| Model | Classes | Dataset | Top-1 Acc | Top-3 Acc | Notes |
|-------|---------|---------|-----------|-----------|-------|
| OpenHands | 20 | Original | 27.6% | - | Baseline |
| OpenHands | 20 | Augmented | TBD | TBD | With augmentation |
| OpenHands | 50 | Augmented | TBD | TBD | 50-class model |

## ğŸ”§ Configuration System

The project uses a centralized configuration system for easy path management:

- **All paths in one place**: `config/settings.json`
- **Auto-detection**: Project root auto-detected
- **Easy migration**: Just update `settings.json` on new machine
- **Cross-platform**: Works on Windows and Linux

See [config/MIGRATION_GUIDE.md](config/MIGRATION_GUIDE.md) for complete documentation.

## ğŸ“¦ Dependencies

Core dependencies:
- Python 3.8+
- PyTorch
- MediaPipe
- OpenCV
- pose-format
- google-generativeai (for Gemini API)
- scikit-learn
- numpy

See `requirements.txt` for complete list.

## ğŸ¤ Contributing

This is a personal research project. If you find issues or have suggestions:

1. Open an issue describing the problem
2. Fork the repository
3. Create a feature branch
4. Submit a pull request

## ğŸ“ Notes

- **Archive folder**: Contains legacy experiments, not actively maintained
- **External datasets**: Store datasets on external drive, configure via `settings.json`
- **Model checkpoints**: Saved in `models/wlasl_XX_class_model/`
- **Training outputs**: Checkpoints support resuming training

## ğŸ“ Citation

If you use this work, please cite:
```
[Your citation information]
```

## ğŸ“„ License

[Your license choice - e.g., MIT, Apache 2.0]

## ğŸ”— Related Projects

- [WLASL Dataset](https://github.com/dxli94/WLASL)
- [MediaPipe](https://google.github.io/mediapipe/)
- [OpenHands](https://github.com/AI4Bharat/OpenHands)

## ğŸ“§ Contact

[Your contact information]

---

**Getting Started Checklist:**

- [ ] Clone repository
- [ ] Install dependencies (`pip install -r requirements.txt`)
- [ ] Configure paths (`python setup_config.py`)
- [ ] Verify configuration (`python -m config.paths`)
- [ ] Prepare dataset in configured location
- [ ] Run training (`python models/training-scripts/train_asl.py`)
- [ ] Test model (`python models/training-scripts/train_asl.py --mode test`)
- [ ] Try webcam inference (`python applications/predict_sentence.py --webcam`)

For detailed setup instructions, see [config/MIGRATION_GUIDE.md](config/MIGRATION_GUIDE.md)
